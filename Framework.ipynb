{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import cv2\n",
    "import csv \n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(r'H:\\Sajal Video\\Models\\model.tf') #load model that was saved in training session\n",
    "\n",
    "#mediapipe settings\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# field names initialization\n",
    "right_wrist_x = right_elbow_x = right_shoulder_x = left_wrist_x = left_elbow_x = left_shoulder_x = right_hip_x = right_knee_x = right_ankle_x = left_hip_x = left_knee_x = left_ankle_x = -1\n",
    "right_wrist_y = right_elbow_y =  right_shoulder_y =  left_wrist_y =  left_elbow_y =  left_shoulder_y = right_hip_y =  right_knee_y = right_ankle_y = left_hip_y = left_knee_y = left_ankle_y = -1\n",
    "\n",
    "# get the video, set camera id for live camera use case, for example set 0 if you have one camera, i.e. web cam\n",
    "cap = cv2.VideoCapture(r'H:\\Sajal Video\\shoplifting\\Pocket 3rd Part.mp4')\n",
    "img_num = 1 #for gathering window_len amount image frame\n",
    "window_len = 32 # set window length as per your traininng\n",
    "rows = [] # list where window_len amount data are stored\n",
    "color = (0,0,255) # color for normal customer\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read() #read the frame\n",
    "        if image is None: #if it is the last frame, break the loop\n",
    "            break\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.,\n",
    "            continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to,\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image_new = image.copy()\n",
    "        image_height, image_width, _ = image_new.shape\n",
    "        if not results.pose_landmarks: #if the image doesn't contain a person, i.e. no landmark is detected then ignore\n",
    "            continue\n",
    "        #get the all necessary landmark coordinates\n",
    "        right_wrist_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST].x * image_width\n",
    "        right_wrist_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST].y * image_height\n",
    "\n",
    "        right_elbow_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW].x * image_width\n",
    "        right_elbow_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW].y * image_height\n",
    "\n",
    "        right_shoulder_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].x * image_width\n",
    "        right_shoulder_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].y * image_height\n",
    "\n",
    "        left_wrist_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST].x * image_width\n",
    "        left_wrist_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST].y * image_height\n",
    "\n",
    "        left_elbow_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW].x * image_width\n",
    "        left_elbow_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW].y * image_height\n",
    "\n",
    "        left_shoulder_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].x * image_width\n",
    "        left_shoulder_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].y * image_height\n",
    "\n",
    "        right_hip_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].x * image_width\n",
    "        right_hip_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].y * image_height\n",
    "\n",
    "        right_knee_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].x * image_width\n",
    "        right_knee_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].y * image_height\n",
    "\n",
    "        right_ankle_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ANKLE].x * image_width\n",
    "        right_ankle_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ANKLE].y * image_height\n",
    "\n",
    "        left_hip_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].x * image_width\n",
    "        left_hip_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].y * image_height\n",
    "\n",
    "        left_knee_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE].x * image_width\n",
    "        left_knee_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE].y * image_height\n",
    "\n",
    "        left_ankle_x = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ANKLE].x * image_width\n",
    "        left_ankle_y = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ANKLE].y * image_height\n",
    "        \n",
    "        \n",
    "        #stack them serially as you have collected the datapoints\n",
    "        row = [\n",
    "            left_shoulder_x, left_shoulder_y,\n",
    "            right_shoulder_x, right_shoulder_y,\n",
    "            left_elbow_x, left_elbow_y,\n",
    "            right_elbow_x, right_elbow_y,\n",
    "            left_wrist_x, left_wrist_y ,\n",
    "            right_wrist_x, right_wrist_y,\n",
    "            left_hip_x, left_hip_y,\n",
    "            right_hip_x, right_hip_y,\n",
    "            left_knee_x, left_knee_y,\n",
    "            right_knee_x,right_knee_y,\n",
    "            left_ankle_x, left_ankle_y,\n",
    "            right_ankle_x, right_ankle_y,\n",
    "            \n",
    "            \n",
    "        ]\n",
    "        \"\"\"\n",
    "        row = [\n",
    "            right_wrist_x, right_wrist_y,\n",
    "            right_elbow_x, right_elbow_y,\n",
    "            right_shoulder_x, right_shoulder_y,\n",
    "            left_wrist_x, left_wrist_y ,\n",
    "            left_elbow_x, left_elbow_y,\n",
    "            left_shoulder_x, left_shoulder_y,\n",
    "            right_hip_x, right_hip_y,\n",
    "            right_knee_x,right_knee_y,\n",
    "            right_ankle_x, right_ankle_y,\n",
    "            left_hip_x, left_hip_y,\n",
    "            left_knee_x, left_knee_y,\n",
    "            left_ankle_x, left_ankle_y\n",
    "        ]\n",
    "        \"\"\"\n",
    "        rows.append(row)\n",
    "        cv2.rectangle(image, (int(right_shoulder_x),int(right_shoulder_y)), (int(left_ankle_x), int(left_ankle_y)), color, 2) #create a bounding box around the customer\n",
    "        if np.array(rows).shape[0] % window_len == 0: #if window length amount data is collected\n",
    "            rows_2 = rows",
    "            rows = [rows] # for formating purpose\n",
    "            X_test = np.array(rows) #converting into numpy array for feeding it into the model properly\n",
    "            del rows_2[0]# discarding the first item of the list for next window\n",
    "            rows = rows_2",
    "            out = model.predict(X_test) #get the prediction\n",
    "            print(np.argmax(out, axis=1)[0]) \n",
    "            if np.argmax(out, axis=1)[0] == 1:#if the prediction is 1, change the color to Red\n",
    "                color = (255,0,0)\n",
    "            if np.argmax(out, axis=1)[0] == 0: #if it is 0, change back the color to green\n",
    "                color = (0,0,255)\n",
    "        img_num = img_num + 1 #count the number of frames passed\n",
    "\n",
    "        # Draw the pose annotation on the image.,\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "        # Flip the image horizontally for a selfie-view display.,\n",
    "        cv2.imshow('MediaPipe Pose', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(5) & 0xFF == 27: #break when escape button is pressed\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
